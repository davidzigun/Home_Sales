{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KW73O2e3dw",
        "outputId": "78e8960b-ed5a-4712-e543-8384fdfe2762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2XbWNf1Te5fM"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wOJqxG_RPSwp"
      },
      "outputs": [],
      "source": [
        "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a datafram of the URL using Sparkfiles\n",
        "spark.sparkContext.addFile(url)\n",
        "home_sales_df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "9EoIY279thbK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RoljcJ7WPpnm"
      },
      "outputs": [],
      "source": [
        "# 2. Create a temporary view of the DataFrame called home_sales\n",
        "home_sales_df.createOrReplaceTempView('home_sales')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#show header of first 5 rows of temporary view home_sales\n",
        "spark.sql(\"select * from home_sales limit 5\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftP0qo2ouOle",
        "outputId": "487b7aa7-bfae-4d49-e3c0-28fff0eeff28"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
            "|7530a2d8-1ae3-451...|2021-06-13|      2013|379628|       2|        2|       2235|   14384|     1|         0|  23|\n",
            "|43de979c-0bf0-4c9...|2019-04-12|      2014|417866|       2|        2|       2127|   10575|     2|         0|   0|\n",
            "|b672c137-b88c-48b...|2019-10-16|      2016|239895|       2|        2|       1631|   11149|     2|         0|   0|\n",
            "|e0726d4d-d595-407...|2022-01-08|      2017|424418|       3|        2|       2249|   13878|     2|         0|   4|\n",
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places\n",
        "#filter to rows with \"bedrooms\" = 4\n",
        "home_sales_3_df = home_sales_df.filter(home_sales_df[\"bedrooms\"] == 4) #Assign filtered DataFrame to home_sales_3_df\n",
        "home_sales_3_df.show() # Show the results of the filtered DataFrame\n",
        "# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places\n",
        "#Extract first 4 characters of \"date\" and alias column as \"year\"\n",
        "#Extract first 4 characters of \"date_built\" and alias column as \"year_built\"\n",
        "from pyspark.sql.functions import year\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "# Cast the \"date_built\" column to StringType before applying the year function\n",
        "home_sales_3_df = home_sales_3_df.withColumn(\"date_str\", home_sales_3_df[\"date\"].cast(StringType())) #Keep home_sales_3_df as a Spark DataFrame\n",
        "home_sales_3_df = home_sales_3_df.withColumn(\"year\", year(home_sales_3_df[\"date_str\"]))\n",
        "#Show unique values of year within home_sales_df\n",
        "home_sales_3_df.select(\"year\").distinct().show()\n",
        "\n",
        "\n",
        "#Calculate mean of price rounded to 2 decimals, grouping by \"year\"\n",
        "from pyspark.sql.functions import mean, round\n",
        "avg_price_per_year = home_sales_3_df.groupBy(\"year\").agg(round(mean(\"price\"), 2).alias(\"mean_price\"))\n",
        "print(avg_price_per_year.show())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz3xssev6uAM",
        "outputId": "539589c4-8bb4-4bae-f43d-8335abb88e11"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|f8a53099-ba1c-47d...|2022-04-08|      2016|936923|       4|        3|       3167|   11733|     2|         1|  76|\n",
            "|f233cb41-6f33-4b0...|2021-07-18|      2016|437375|       4|        3|       1704|   11721|     2|         0|  34|\n",
            "|2dc662fe-57b8-44b...|2020-04-20|      2016|417215|       4|        2|       2104|    8227|     2|         0|  48|\n",
            "|d5f2062a-8950-417...|2019-09-02|      2016|305980|       4|        2|       1799|   13004|     1|         0|  26|\n",
            "|f5e01433-f7b0-44e...|2021-03-10|      2010|335326|       4|        3|       1588|   14107|     1|         0|   8|\n",
            "|a818abe9-fec6-48d...|2019-08-20|      2017|334284|       4|        2|       2378|   12034|     2|         0|  38|\n",
            "|513c8016-c85b-4f6...|2019-04-13|      2016|138006|       4|        2|       1780|   14211|     2|         0|  12|\n",
            "|7ac67498-b6f3-403...|2021-05-12|      2015|349318|       4|        3|       2417|   11304|     2|         0|  37|\n",
            "|5a55a233-f534-407...|2020-07-21|      2011|288365|       4|        2|       1643|   14886|     2|         0|  29|\n",
            "|ae3b0e50-4238-421...|2019-10-13|      2013|299056|       4|        3|       1613|   11634|     2|         0|  17|\n",
            "|f99463dd-c04f-4c4...|2020-08-31|      2014|232702|       4|        2|       1717|    9138|     1|         0|  19|\n",
            "|ffc3b5af-2180-445...|2020-07-20|      2012|301886|       4|        2|       2387|   10259|     2|         0|  41|\n",
            "|ddf82ffe-df12-477...|2020-02-06|      2014|268775|       4|        3|       2413|    8383|     2|         0|   8|\n",
            "|be0ccb95-415d-411...|2020-05-15|      2015|425154|       4|        3|       2120|   14229|     2|         0|   4|\n",
            "|1ce21d55-2744-432...|2022-03-30|      2014|266254|       4|        2|       1661|   11223|     1|         0|  40|\n",
            "|8e05076c-0c64-429...|2022-06-03|      2011|381358|       4|        3|       2190|   13460|     1|         0|  46|\n",
            "|fd8213d0-3872-4de...|2020-04-23|      2017|858900|       4|        5|       4465|    9873|     1|         1|  53|\n",
            "|e9031a86-1294-444...|2021-10-09|      2015|222322|       4|        3|       1928|   10510|     1|         0|  38|\n",
            "|731d14ac-9dd1-4e5...|2021-03-27|      2011|323749|       4|        2|       2427|   14387|     2|         0|  21|\n",
            "|e6d7c2a7-596e-4ec...|2019-03-15|      2015|131201|       4|        3|       1633|   14655|     1|         0|  22|\n",
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----+\n",
            "|year|\n",
            "+----+\n",
            "|2022|\n",
            "|2019|\n",
            "|2020|\n",
            "|2021|\n",
            "+----+\n",
            "\n",
            "+----+----------+\n",
            "|year|mean_price|\n",
            "+----+----------+\n",
            "|2022| 296363.88|\n",
            "|2019|  300263.7|\n",
            "|2020| 298353.78|\n",
            "|2021| 301819.44|\n",
            "+----+----------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# 4. What is the average price of a home for each year the home was built,that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?\n",
        "#filter to rows with \"bedrooms\" = 3 and \"bathrooms\" == 3\n",
        "# Corrected: Assign the filtered DataFrame to home_sales_4_df instead of calling show()\n",
        "home_sales_4_df = home_sales_df.filter((home_sales_df[\"bedrooms\"] == 3) & (home_sales_df[\"bathrooms\"] == 3))\n",
        "home_sales_4_df.show()\n",
        "#Extract first 4 characters of \"date_built\" and alias column as \"year_built\"\n",
        "from pyspark.sql.functions import year\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "# Cast the \"date_built\" column to StringType before applying the year function\n",
        "home_sales_4_df = home_sales_4_df.withColumn(\"date_built_str\", home_sales_4_df[\"date_built\"].cast(StringType()))\n",
        "home_sales_4_df = home_sales_4_df.withColumn(\"year_built\", year(home_sales_4_df[\"date_built_str\"]))\n",
        "\n",
        "\n",
        "#Show unique values of year within home_sales_df\n",
        "home_sales_4_df.select(\"year_built\").distinct().show()\n",
        "\n",
        "\n",
        "#Calculate mean of price rounded to 2 decimals, grouping by \"year\"\n",
        "from pyspark.sql.functions import mean, round\n",
        "avg_price_per_year_built = home_sales_4_df.groupBy(\"year_built\").agg(round(mean(\"price\"), 2).alias(\"mean_price\"))\n",
        "print(avg_price_per_year_built.show())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbbP6iQB7bIv",
        "outputId": "7e7842c8-0115-4e5b-9f6b-70553f9dff58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|                  id|      date|date_built| price|bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|e81aacfe-17fe-46b...|2020-06-16|      2016|181925|       3|        3|       2137|   11709|     2|         0|  22|\n",
            "|2ed8d509-7372-46d...|2021-08-06|      2015|258710|       3|        3|       1918|    9666|     1|         0|  25|\n",
            "|f876d86f-3c9f-42b...|2019-02-27|      2011|167864|       3|        3|       2471|   13924|     2|         0|  15|\n",
            "|941bad30-eb49-4a7...|2020-05-09|      2015|229896|       3|        3|       2197|    8641|     1|         0|   3|\n",
            "|ea620c7b-c2f7-4c6...|2021-05-31|      2011|437958|       3|        3|       2356|   11052|     1|         0|  26|\n",
            "|0cfe57f3-28c2-472...|2019-10-04|      2015|308313|       3|        3|       1960|    9453|     2|         0|   2|\n",
            "|4566cd2a-ac6e-435...|2019-07-15|      2016|177541|       3|        3|       2130|   10517|     2|         0|  25|\n",
            "|50873b8b-92b8-44c...|2022-06-05|      2017|268554|       3|        3|       1962|   13955|     2|         0|   6|\n",
            "|e0fc52aa-c349-4ba...|2019-04-17|      2016|202790|       3|        3|       2025|   10987|     2|         0|  19|\n",
            "|e252c4ce-b5b2-4ef...|2019-10-28|      2014|168463|       3|        3|       2271|    8317|     2|         0|   2|\n",
            "|a16c1fd3-50c3-4e7...|2020-02-12|      2014|372151|       3|        3|       1787|   12012|     2|         0|  16|\n",
            "|0ffe23e3-9d0f-442...|2021-11-18|      2017|429183|       3|        3|       2162|   12567|     1|         0|  49|\n",
            "|bb2ff269-f08f-4e8...|2022-03-06|      2010|429030|       3|        3|       2049|   11882|     2|         0|  23|\n",
            "|77a2d239-67a3-43b...|2021-06-18|      2016|396508|       3|        3|       1890|   10081|     2|         0|  25|\n",
            "|34c31a34-220d-469...|2019-02-06|      2015|409011|       3|        3|       2356|   10507|     1|         0|   1|\n",
            "|7acb88a1-8a4f-4a7...|2019-03-31|      2016|351363|       3|        3|       1771|   13037|     2|         0|  13|\n",
            "|d03391c1-cfd4-4d1...|2022-04-01|      2011|158791|       3|        3|       2371|   12585|     2|         0|  23|\n",
            "|0c686512-f14e-450...|2022-06-12|      2013|391189|       3|        3|       1552|    9659|     2|         0|  31|\n",
            "|24ba1e84-88a6-4e3...|2019-09-17|      2017|207869|       3|        3|       1768|   11426|     1|         0|  12|\n",
            "|00ce46df-952c-4ce...|2020-05-11|      2016|430471|       3|        3|       1935|   13334|     2|         0|  10|\n",
            "+--------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----------+\n",
            "|year_built|\n",
            "+----------+\n",
            "|      2015|\n",
            "|      2013|\n",
            "|      2014|\n",
            "|      2012|\n",
            "|      2016|\n",
            "|      2010|\n",
            "|      2011|\n",
            "|      2017|\n",
            "+----------+\n",
            "\n",
            "+----------+----------+\n",
            "|year_built|mean_price|\n",
            "+----------+----------+\n",
            "|      2015|  288770.3|\n",
            "|      2013| 295962.27|\n",
            "|      2014| 290852.27|\n",
            "|      2012| 293683.19|\n",
            "|      2016| 290555.07|\n",
            "|      2010| 292859.62|\n",
            "|      2011| 291117.47|\n",
            "|      2017| 292676.79|\n",
            "+----------+----------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# 5. What is the average price of a home for each year the home was built,\n",
        "# that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet, rounded to two decimal places?\n",
        "#filter to rows with \"bedrooms\" = 3 and \"bathrooms\" == 3 and \"floors\" == 2 $gte 2,000 Sqr Ft\n",
        "#Assign the filtered DataFrame to home_sales_5_df and remove .show()\n",
        "home_sales_5_df = home_sales_df.filter((home_sales_df[\"bedrooms\"] == 3) & (home_sales_df[\"bathrooms\"] == 3) & (home_sales_df[\"floors\"] == 2) & (home_sales_df[\"sqft_living\"] >= 2000))\n",
        "\n",
        "#Extract first 4 characters of \"date_built\" and alias column as \"year_built\"\n",
        "from pyspark.sql.functions import year\n",
        "from pyspark.sql.types import StringType, IntegerType\n",
        "# Cast the \"date_built\" column to StringType before applying the year function\n",
        "#Use home_sales_5_df instead of home_sales_df here\n",
        "home_sales_5_df = home_sales_5_df.withColumn(\"date_built_str\", home_sales_5_df[\"date_built\"].cast(StringType()))\n",
        "#Use home_sales_5_df instead of home_sales_df here\n",
        "home_sales_5_df = home_sales_5_df.withColumn(\"year_built\", year(home_sales_5_df[\"date_built_str\"]))\n",
        "\n",
        "#Show unique values of year within home_sales_df\n",
        "home_sales_5_df.select(\"year_built\").distinct().show()\n",
        "\n",
        "#Calculate mean of price rounded to 2 decimals, grouping by \"year\"\n",
        "from pyspark.sql.functions import mean, round\n",
        "avg_price_per_year_built = home_sales_5_df.groupBy(\"year_built\").agg(round(mean(\"price\"), 2).alias(\"mean_price\"))\n",
        "print(avg_price_per_year_built.show())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UILXxNn8_DA2",
        "outputId": "c6131032-d618-443b-d8b3-13f57e865a69"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|year_built|\n",
            "+----------+\n",
            "|      2015|\n",
            "|      2013|\n",
            "|      2014|\n",
            "|      2012|\n",
            "|      2016|\n",
            "|      2010|\n",
            "|      2011|\n",
            "|      2017|\n",
            "+----------+\n",
            "\n",
            "+----------+----------+\n",
            "|year_built|mean_price|\n",
            "+----------+----------+\n",
            "|      2015| 297609.97|\n",
            "|      2013| 303676.79|\n",
            "|      2014| 298264.72|\n",
            "|      2012| 307539.97|\n",
            "|      2016|  293965.1|\n",
            "|      2010| 285010.22|\n",
            "|      2011| 276553.81|\n",
            "|      2017| 280317.58|\n",
            "+----------+----------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUrfgOX1pCRd",
        "outputId": "23827115-554c-45ed-bc2c-aff8bf9eacda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|view|mean_price|\n",
            "+----+----------+\n",
            "| 100| 1026669.5|\n",
            "|  99|1061201.42|\n",
            "|  98|1053739.33|\n",
            "|  97|1129040.15|\n",
            "|  96|1017815.92|\n",
            "|  95| 1054325.6|\n",
            "|  94| 1033536.2|\n",
            "|  93|1026006.06|\n",
            "|  92| 970402.55|\n",
            "|  91|1137372.73|\n",
            "|  90|1062654.16|\n",
            "|  89|1107839.15|\n",
            "|  88|1031719.35|\n",
            "|  87| 1072285.2|\n",
            "|  86|1070444.25|\n",
            "|  85|1056336.74|\n",
            "|  84|1117233.13|\n",
            "|  83|1033965.93|\n",
            "|  82| 1063498.0|\n",
            "|  81|1053472.79|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 9.751319885253906e-05 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 6. What is the average price of a home per \"view\" rating, rounded to two decimal places, having an average home price greater than or equal to $350,000? Order by descending view rating.\n",
        "# Although this is a small dataset, determine the run time for this query.\n",
        "#Filter home_sales_df for for price greater or equal to $350,000\n",
        "home_sales_6_df = home_sales_df.filter(home_sales_df[\"price\"] >= 350000)\n",
        "\n",
        "#aggregate mean price of homes in home_sales_df by \"view\", sorting in descending order of 'view'\n",
        "from pyspark.sql.functions import mean, round\n",
        "avg_price_per_view = home_sales_6_df.groupBy(\"view\").agg(round(mean(\"price\"), 2).alias(\"mean_price\"))\n",
        "avg_price_per_view.sort(avg_price_per_view[\"view\"].desc()).show()\n",
        "\n",
        "\n",
        "#calculate start time and end time of calculation in seconds\n",
        "start_time = time.time()\n",
        "end_time = time.time() - start_time\n",
        "print(\"--- %s seconds ---\" % (end_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KAhk3ZD2tFy8"
      },
      "outputs": [],
      "source": [
        "# 7. Cache the the temporary table home_sales.\n",
        "spark.catalog.cacheTable('home_sales')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4opVhbvxtL-i"
      },
      "outputs": [],
      "source": [
        "# 8. Check if the table is cached.\n",
        "spark.catalog.isCached('home_sales')\n",
        "#assign cached home_sales to new df\n",
        "home_sales_cache_df = spark.table(\"home_sales\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GnL46lwTSEk",
        "outputId": "7b2aaa04-ba4f-4aba-b962-8ccb482c725e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|view|mean_price|\n",
            "+----+----------+\n",
            "| 100| 1026669.5|\n",
            "|  99|1061201.42|\n",
            "|  98|1053739.33|\n",
            "|  97|1129040.15|\n",
            "|  96|1017815.92|\n",
            "|  95| 1054325.6|\n",
            "|  94| 1033536.2|\n",
            "|  93|1026006.06|\n",
            "|  92| 970402.55|\n",
            "|  91|1137372.73|\n",
            "|  90|1062654.16|\n",
            "|  89|1107839.15|\n",
            "|  88|1031719.35|\n",
            "|  87| 1072285.2|\n",
            "|  86|1070444.25|\n",
            "|  85|1056336.74|\n",
            "|  84|1117233.13|\n",
            "|  83|1033965.93|\n",
            "|  82| 1063498.0|\n",
            "|  81|1053472.79|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 8.726119995117188e-05 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 9. Using the cached data, run the last query above, that calculates the average price of a home per \"view\" rating, rounded to two decimal places, having an average home price greater than or equal to $350,000.\n",
        "#Run avg_price_per_view using the filters of home_sales_6_df on the chached home_sales table\n",
        "#Filter home_sales_df for for price greater or equal to $350,000\n",
        "home_sales_6cache_df = home_sales_cache_df.filter(home_sales_cache_df[\"price\"] >= 350000)\n",
        "\n",
        "from pyspark.sql.functions import mean, round\n",
        "avg_price_per_view = home_sales_6cache_df.groupBy(\"view\").agg(round(mean(\"price\"), 2).alias(\"mean_price\"))\n",
        "avg_price_per_view.sort(avg_price_per_view[\"view\"].desc()).show()\n",
        "\n",
        "# Determine the runtime and compare it to the uncached runtime.\n",
        "#run avg_price_per_view using cacheTabe('home_sales')\n",
        "\n",
        "start_time = time.time()\n",
        "end_time = time.time() - start_time\n",
        "print(\"--- %s seconds ---\" % (end_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Qm12WN9isHBR"
      },
      "outputs": [],
      "source": [
        "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
        "home_sales_df.write.partitionBy(\"date_built\").mode(\"overwrite\").parquet(\"home_sales_partitioned\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AZ7BgY61sRqY"
      },
      "outputs": [],
      "source": [
        "# 11. Read the parquet formatted data.\n",
        "home_sales_partitioned_df = spark.read.parquet(\"home_sales_partitioned\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "J6MJkHfvVcvh"
      },
      "outputs": [],
      "source": [
        "# 12. Create a temporary table for the parquet data.\n",
        "home_sales_partitioned_df.createOrReplaceTempView('home_sales_partitioned')\n",
        "home_sales_partitioned_df = spark.table(\"home_sales_partitioned\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Vhb52rU1Sn",
        "outputId": "babe2731-0d4d-4bbf-cdf1-1cd8722a3d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|view|mean_price|\n",
            "+----+----------+\n",
            "| 100| 1026669.5|\n",
            "|  99|1061201.42|\n",
            "|  98|1053739.33|\n",
            "|  97|1129040.15|\n",
            "|  96|1017815.92|\n",
            "|  95| 1054325.6|\n",
            "|  94| 1033536.2|\n",
            "|  93|1026006.06|\n",
            "|  92| 970402.55|\n",
            "|  91|1137372.73|\n",
            "|  90|1062654.16|\n",
            "|  89|1107839.15|\n",
            "|  88|1031719.35|\n",
            "|  87| 1072285.2|\n",
            "|  86|1070444.25|\n",
            "|  85|1056336.74|\n",
            "|  84|1117233.13|\n",
            "|  83|1033965.93|\n",
            "|  82| 1063498.0|\n",
            "|  81|1053472.79|\n",
            "+----+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 8.726119995117188e-05 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 13. Using the parquet DataFrame, run the last query above, that calculates\n",
        "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
        "# having an average home price greater than or equal to $350,000.\n",
        "# Determine the runtime and compare it to the cached runtime.\n",
        "home_sales_6part_df = home_sales_partitioned_df.filter(home_sales_partitioned_df[\"price\"] >= 350000)\n",
        "\n",
        "from pyspark.sql.functions import mean, round\n",
        "avg_price_per_view = home_sales_6part_df.groupBy(\"view\").agg(round(mean(\"price\"), 2).alias(\"mean_price\"))\n",
        "avg_price_per_view.sort(avg_price_per_view[\"view\"].desc()).show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (end_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hjjYzQGjtbq8"
      },
      "outputs": [],
      "source": [
        "# 14. Uncache the home_sales temporary table.\n",
        "spark.catalog.uncacheTable('home_sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Sy9NBvO7tlmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a262ace2-d276-49e0-ed16-a0a2e11193a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# 15. Check if the home_sales is no longer cached\n",
        "spark.catalog.isCached('home_sales')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Si-BNruRUGK3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}